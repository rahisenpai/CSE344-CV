{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11473251,"sourceType":"datasetVersion","datasetId":7190399}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install open_clip_torch -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T13:34:29.966305Z","iopub.execute_input":"2025-04-19T13:34:29.966625Z","iopub.status.idle":"2025-04-19T13:36:00.253101Z","shell.execute_reply.started":"2025-04-19T13:34:29.966594Z","shell.execute_reply":"2025-04-19T13:36:00.251191Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport torch\nimport torch.nn.functional as F\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\nfrom transformers import CLIPProcessor, CLIPModel\nfrom open_clip import create_model_from_pretrained, get_tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T13:38:18.894061Z","iopub.execute_input":"2025-04-19T13:38:18.894466Z","iopub.status.idle":"2025-04-19T13:38:18.900079Z","shell.execute_reply.started":"2025-04-19T13:38:18.894440Z","shell.execute_reply":"2025-04-19T13:38:18.898906Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## BLIP","metadata":{}},{"cell_type":"code","source":"model_name = \"Salesforce/blip-image-captioning-large\"\nprocessor = BlipProcessor.from_pretrained(model_name)\nmodel = BlipForConditionalGeneration.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T13:36:43.735133Z","iopub.execute_input":"2025-04-19T13:36:43.735438Z","iopub.status.idle":"2025-04-19T13:36:55.594116Z","shell.execute_reply.started":"2025-04-19T13:36:43.735415Z","shell.execute_reply":"2025-04-19T13:36:55.593359Z"}},"outputs":[{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/445 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f90b499df87847409636c50e437f1dae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/527 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc12e1bdfa1c481f8a4bad0b1c420c39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2a20166dbd146018e1c47351a7b5989"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb588a1eadc9444eb6d2f875792363ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87fbb1d90b244af3bc2698a2617001c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.60k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3afe975b9e1448d7a2c5b7da431681b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.88G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7934c4e4846b4ddda882328601cc2f4a"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"base = \"/kaggle/input/cse344-a3/samples\"\nimages = [{\"name\":img, \"image\":Image.open(os.path.join(base,img)).convert(\"RGB\")} for img in os.listdir(base)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T13:36:55.595254Z","iopub.execute_input":"2025-04-19T13:36:55.596094Z","iopub.status.idle":"2025-04-19T13:36:55.710683Z","shell.execute_reply.started":"2025-04-19T13:36:55.596058Z","shell.execute_reply":"2025-04-19T13:36:55.709832Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"for img_dict in images:\n    inputs = processor(img_dict[\"image\"], return_tensors=\"pt\")\n    out = model.generate(**inputs)\n    img_dict[\"blip\"] = processor.decode(out[0], skip_special_tokens=True)\n    print(f\"{img_dict['name']} : {img_dict['blip']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T13:39:34.535374Z","iopub.execute_input":"2025-04-19T13:39:34.536246Z","iopub.status.idle":"2025-04-19T13:40:53.674930Z","shell.execute_reply.started":"2025-04-19T13:39:34.536202Z","shell.execute_reply":"2025-04-19T13:40:53.673745Z"}},"outputs":[{"name":"stdout","text":"ILSVRC2012_test_00000004.jpg : arafed dog running in a field of grass with a sky background\nILSVRC2012_test_00000022.jpg : there is a small dog standing on a ledge near a pool\nILSVRC2012_test_00000023.jpg : they are riding on a bike in the rain in the street\nILSVRC2012_test_00000026.jpg : arafed man in a suit and tie sitting on a couch\nILSVRC2012_test_00000018.jpg : there are four children sitting on a towel by a pool\nILSVRC2012_test_00000003.jpg : araffe dog running on a leash at a dog show\nILSVRC2012_test_00000019.jpg : there is a bird that is sitting on a plant with green leaves\nILSVRC2012_test_00000030.jpg : ducks are standing in the water and one is drinking\nILSVRC2012_test_00000034.jpg : two cups of coffee being poured into a coffee machine\nILSVRC2012_test_00000025.jpg : there is a brown butterfly sitting on a leaf in the grass\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## CLIP","metadata":{}},{"cell_type":"code","source":"clip_model_name = \"openai/clip-vit-base-patch32\"\nclip_model = CLIPModel.from_pretrained(clip_model_name)\nclip_processor = CLIPProcessor.from_pretrained(clip_model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T13:40:53.676651Z","iopub.execute_input":"2025-04-19T13:40:53.676999Z","iopub.status.idle":"2025-04-19T13:41:00.250073Z","shell.execute_reply.started":"2025-04-19T13:40:53.676976Z","shell.execute_reply":"2025-04-19T13:41:00.248869Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f07a65d13cfc4e77a2759b6c42aa095d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef731bac1dc94a7aa2fdcd39c32cc73e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3dc1fcf420e64999a89863f0109a79df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efb84bc6f0e14b75bcd92fd61de244ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b299a9987ec441c9b6087f5b59e506b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f95f7bbe30be48c2b263948441e9f802"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b78b0b2fb3584bec83517d5c8d3caf79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f17ddf1dc2843778a91099bcb2a2545"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b05b52bb3744c6f8b9d0fdaa254e6f9"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"for img_dict in images:\n    inputs = clip_processor(text=img_dict[\"blip\"], images=img_dict[\"image\"], return_tensors=\"pt\", padding=True)\n    with torch.no_grad():\n        outputs = clip_model(**inputs)\n        logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n    img_dict[\"clip\"] = logits_per_image[0][0].item()\n    print(f\"{img_dict['name']} : {img_dict['blip']},  {img_dict['clip']:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T13:42:27.736627Z","iopub.execute_input":"2025-04-19T13:42:27.736995Z","iopub.status.idle":"2025-04-19T13:42:29.472552Z","shell.execute_reply.started":"2025-04-19T13:42:27.736964Z","shell.execute_reply":"2025-04-19T13:42:29.471452Z"}},"outputs":[{"name":"stdout","text":"ILSVRC2012_test_00000004.jpg : arafed dog running in a field of grass with a sky background,  30.7687\nILSVRC2012_test_00000022.jpg : there is a small dog standing on a ledge near a pool,  30.2497\nILSVRC2012_test_00000023.jpg : they are riding on a bike in the rain in the street,  32.4249\nILSVRC2012_test_00000026.jpg : arafed man in a suit and tie sitting on a couch,  28.6873\nILSVRC2012_test_00000018.jpg : there are four children sitting on a towel by a pool,  33.6626\nILSVRC2012_test_00000003.jpg : araffe dog running on a leash at a dog show,  33.2162\nILSVRC2012_test_00000019.jpg : there is a bird that is sitting on a plant with green leaves,  26.5880\nILSVRC2012_test_00000030.jpg : ducks are standing in the water and one is drinking,  30.5419\nILSVRC2012_test_00000034.jpg : two cups of coffee being poured into a coffee machine,  30.2205\nILSVRC2012_test_00000025.jpg : there is a brown butterfly sitting on a leaf in the grass,  29.0515\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"## CLIPS","metadata":{}},{"cell_type":"code","source":"clips_model, clips_preprocess = create_model_from_pretrained('hf-hub:UCSC-VLAA/ViT-L-14-CLIPS-224-Recap-DataComp-1B')\nclips_tokenizer = get_tokenizer('hf-hub:UCSC-VLAA/ViT-L-14-CLIPS-224-Recap-DataComp-1B')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T13:41:16.994982Z","iopub.execute_input":"2025-04-19T13:41:16.995326Z","iopub.status.idle":"2025-04-19T13:41:36.094375Z","shell.execute_reply.started":"2025-04-19T13:41:16.995303Z","shell.execute_reply":"2025-04-19T13:41:36.093396Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"open_clip_pytorch_model.bin:   0%|          | 0.00/1.66G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be53d8a1d39946f28a8baae3530a0be7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"open_clip_config.json:   0%|          | 0.00/943 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cdc6075ce0144ccb3776557589a476f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffdd074c3e594e3da2b0cbe10b5925fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d048ea488844dcbbcc92964c9d2a2a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d251fab46014c5ea8b5494d2a209525"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3f745ffeab24a7bb08f9d3f5cae0a64"}},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"for img_dict in images:\n    image = clips_preprocess(img_dict[\"image\"]).unsqueeze(0)\n    text = clips_tokenizer(img_dict[\"blip\"], context_length=clips_model.context_length)\n    \n    with torch.no_grad(), torch.amp.autocast('cuda'):\n        image_features = clips_model.encode_image(image)\n        text_features = clips_model.encode_text(text)\n        image_features = F.normalize(image_features, dim=-1)\n        text_features = F.normalize(text_features, dim=-1)\n        similarity_scores = (100.0 * image_features @ text_features.T) #unnormalized similarity scores\n\n    img_dict[\"clips\"] = similarity_scores[0][0].item()\n    print(f\"{img_dict['name']} : {img_dict['blip']},  {img_dict['clips']:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T13:42:29.473868Z","iopub.execute_input":"2025-04-19T13:42:29.474242Z","iopub.status.idle":"2025-04-19T13:42:49.383945Z","shell.execute_reply.started":"2025-04-19T13:42:29.474208Z","shell.execute_reply":"2025-04-19T13:42:49.382902Z"}},"outputs":[{"name":"stdout","text":"ILSVRC2012_test_00000004.jpg : arafed dog running in a field of grass with a sky background,  15.5006\nILSVRC2012_test_00000022.jpg : there is a small dog standing on a ledge near a pool,  14.5663\nILSVRC2012_test_00000023.jpg : they are riding on a bike in the rain in the street,  17.3345\nILSVRC2012_test_00000026.jpg : arafed man in a suit and tie sitting on a couch,  13.4429\nILSVRC2012_test_00000018.jpg : there are four children sitting on a towel by a pool,  19.8995\nILSVRC2012_test_00000003.jpg : araffe dog running on a leash at a dog show,  17.5147\nILSVRC2012_test_00000019.jpg : there is a bird that is sitting on a plant with green leaves,  9.3397\nILSVRC2012_test_00000030.jpg : ducks are standing in the water and one is drinking,  15.8904\nILSVRC2012_test_00000034.jpg : two cups of coffee being poured into a coffee machine,  15.4525\nILSVRC2012_test_00000025.jpg : there is a brown butterfly sitting on a leaf in the grass,  12.2293\n","output_type":"stream"}],"execution_count":17}]}