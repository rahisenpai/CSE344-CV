{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11473251,"sourceType":"datasetVersion","datasetId":7190399}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## CLIP","metadata":{}},{"cell_type":"code","source":"from transformers import CLIPProcessor, CLIPModel\nfrom PIL import Image\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T12:44:32.773784Z","iopub.execute_input":"2025-04-19T12:44:32.774651Z","iopub.status.idle":"2025-04-19T12:44:32.779214Z","shell.execute_reply.started":"2025-04-19T12:44:32.774620Z","shell.execute_reply":"2025-04-19T12:44:32.778382Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"clip_model_name = \"openai/clip-vit-base-patch32\"\nclip_model = CLIPModel.from_pretrained(clip_model_name)\nclip_processor = CLIPProcessor.from_pretrained(clip_model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T12:36:17.436634Z","iopub.execute_input":"2025-04-19T12:36:17.437154Z","iopub.status.idle":"2025-04-19T12:36:24.590823Z","shell.execute_reply.started":"2025-04-19T12:36:17.437110Z","shell.execute_reply":"2025-04-19T12:36:24.589851Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2082894c66f44423aaa766c2881e141a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f30b5db8beb344ccac32f3b0c05341b7"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09ea288e508a46c785da844440712160"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78a25b966f2e455e9256335db9eab01c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b15eb77e82664bedb4e75cb36b293414"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e60ec754aa04875a052644ae9733f53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbf32e3616944cd3abadd889a70dc89c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfcbbb5e3f7e4c849b9065ca09a8cf05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38635b088ad84c1fb3a1c5ed89a83d3d"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"img = Image.open(\"/kaggle/input/cse344-a3/sample_image.jpg\").convert(\"RGB\")\n\ntexts = [\n    \"a person holding a dog\",\n    \"a person with their pet dog\",\n    \"a person and a furry friend\",\n    \"a man holding a dog\",\n    \"a man with his pet dog\",\n    \"a man and a furry friend\",\n    \"a dog with its owner\",\n    \"a dog with its loving owner\",\n    \"a loyal dog with its owner\",\n    \"a loyal dog with its loving owner\",\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T12:36:24.591773Z","iopub.execute_input":"2025-04-19T12:36:24.591996Z","iopub.status.idle":"2025-04-19T12:36:24.757287Z","shell.execute_reply.started":"2025-04-19T12:36:24.591979Z","shell.execute_reply":"2025-04-19T12:36:24.756486Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"inputs = clip_processor(text=texts, images=img, return_tensors=\"pt\", padding=True)\nwith torch.no_grad():\n    outputs = clip_model(**inputs)\n    logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n\nprint(\"Similarity Scores:\")\nfor i, text in enumerate(texts):\n    score = logits_per_image[0][i].item()\n    print(f\"'{text}': {score:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T12:36:24.758180Z","iopub.execute_input":"2025-04-19T12:36:24.758455Z","iopub.status.idle":"2025-04-19T12:36:25.266238Z","shell.execute_reply.started":"2025-04-19T12:36:24.758433Z","shell.execute_reply":"2025-04-19T12:36:25.265421Z"}},"outputs":[{"name":"stdout","text":"Similarity Scores:\n'a person holding a dog': 29.7640\n'a person with their pet dog': 29.3991\n'a person and a furry friend': 26.4187\n'a man holding a dog': 30.5882\n'a man with his pet dog': 29.9326\n'a man and a furry friend': 28.3417\n'a dog with its owner': 27.9548\n'a dog with its loving owner': 26.8501\n'a loyal dog with its owner': 25.7771\n'a loyal dog with its loving owner': 25.1614\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## CLIPS","metadata":{}},{"cell_type":"code","source":"!pip install open_clip_torch -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T12:36:25.267066Z","iopub.execute_input":"2025-04-19T12:36:25.267489Z","iopub.status.idle":"2025-04-19T12:37:52.917037Z","shell.execute_reply.started":"2025-04-19T12:36:25.267385Z","shell.execute_reply":"2025-04-19T12:37:52.916103Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch.nn.functional as F\nfrom open_clip import create_model_from_pretrained, get_tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T12:37:52.918533Z","iopub.execute_input":"2025-04-19T12:37:52.918880Z","iopub.status.idle":"2025-04-19T12:37:56.726010Z","shell.execute_reply.started":"2025-04-19T12:37:52.918846Z","shell.execute_reply":"2025-04-19T12:37:56.725161Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"clips_model, clips_preprocess = create_model_from_pretrained('hf-hub:UCSC-VLAA/ViT-L-14-CLIPS-224-Recap-DataComp-1B')\nclips_tokenizer = get_tokenizer('hf-hub:UCSC-VLAA/ViT-L-14-CLIPS-224-Recap-DataComp-1B')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T12:37:56.728475Z","iopub.execute_input":"2025-04-19T12:37:56.728707Z","iopub.status.idle":"2025-04-19T12:38:14.143609Z","shell.execute_reply.started":"2025-04-19T12:37:56.728689Z","shell.execute_reply":"2025-04-19T12:38:14.142841Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"open_clip_pytorch_model.bin:   0%|          | 0.00/1.66G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef65cb676ef5460080985ef852f2c950"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"open_clip_config.json:   0%|          | 0.00/943 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"893cb98a99234889b2d29814b9388331"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"558375253b524e518feeae897c94bed2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b09f9417af764d4f8c950e7a1026473c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5242f17f8ef4de5abce3ed259bbf7a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d08906e4ed24049be47b0f134ec9896"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"image = clips_preprocess(img).unsqueeze(0)\ntext = clips_tokenizer(texts, context_length=clips_model.context_length)\n\nwith torch.no_grad(), torch.amp.autocast('cuda'):\n    image_features = clips_model.encode_image(image)\n    text_features = clips_model.encode_text(text)\n    image_features = F.normalize(image_features, dim=-1)\n    text_features = F.normalize(text_features, dim=-1)\n    similarity_scores = (100.0 * image_features @ text_features.T) #unnormalized similarity scores\n\nprint(\"Similarity Scores:\")\nfor i, text in enumerate(texts):\n    score = similarity_scores[0][i].item()\n    print(f\"'{text}': {score:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T12:41:43.321292Z","iopub.execute_input":"2025-04-19T12:41:43.322020Z","iopub.status.idle":"2025-04-19T12:41:46.627583Z","shell.execute_reply.started":"2025-04-19T12:41:43.321987Z","shell.execute_reply":"2025-04-19T12:41:46.626547Z"}},"outputs":[{"name":"stdout","text":"Similarity Scores:\n'a person holding a dog': 13.2046\n'a person with their pet dog': 12.3108\n'a person and a furry friend': 12.8810\n'a man holding a dog': 16.3031\n'a man with his pet dog': 15.4424\n'a man and a furry friend': 15.2662\n'a dog with its owner': 15.0931\n'a dog with its loving owner': 15.3591\n'a loyal dog with its owner': 14.7291\n'a loyal dog with its loving owner': 15.2724\n","output_type":"stream"}],"execution_count":19}]}